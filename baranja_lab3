1Ô∏è‚É£ prepare_data(df)
üîπ –ë–∞—Ä–∞—ö–∞

–î–∞ —Å–µ –æ—Ç—Å—Ç—Ä–∞–Ω–∞—Ç –∫–æ–ª–æ–Ω–∏—Ç–µ:

RowNumber

CustomerId

Surname

–î–∞ —Å–µ –æ–¥–¥–µ–ª–∞—Ç:

X ‚Üí –∫–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

y ‚Üí —Ü–µ–ª (Exited)

–î–∞ —Å–µ –µ–Ω–∫–æ–¥–∏—Ä–∞–∞—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—ò–∞–ª–Ω–∏—Ç–µ –∫–æ–ª–æ–Ω–∏:

Geography

Gender
—Å–æ OneHotEncoder(drop="first")

–î–∞ —Å–µ –∫–æ—Ä–∏—Å—Ç–∏ ColumnTransformer

–î–∞ —Å–µ —Å–∫–∞–ª–∏—Ä–∞–∞—Ç —Å–∏—Ç–µ –∫–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–æ StandardScaler

–î–∞ —Å–µ –ø–æ–¥–µ–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ—Ç:

80% train / 20% validation

random_state=42

stratify=y

–î–∞ —Å–µ –≤—Ä–∞—Ç–∞—Ç:

X_train, X_val, y_train, y_val, preprocessor, scaler

‚úÖ –®—Ç–æ –Ω–∞–ø—Ä–∞–≤–∏–≤–º–µ

–ì–∏ –∏–∑–±—Ä–∏—à–∞–≤–º–µ –Ω–µ–ø–æ—Ç—Ä–µ–±–Ω–∏—Ç–µ ID –∫–æ–ª–æ–Ω–∏ (–Ω–µ –Ω–æ—Å–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—ò–∞)

Exited –µ –æ–¥–≤–æ–µ–Ω –∫–∞–∫–æ target

Geography –∏ Gender —Å–µ one-hot encoded

–°–∏—Ç–µ –∫–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–µ –ø—Ä–µ—Ç–≤–æ—Ä–µ–Ω–∏ –≤–æ –±—Ä–æ—ò–∫–∏

–°√® –µ —Å–∫–∞–ª–∏—Ä–∞–Ω–æ (NN —Ä–∞–±–æ—Ç–∏ –º–Ω–æ–≥—É –ø–æ–¥–æ–±—Ä–æ —Å–æ —Å–∫–∞–ª–∏—Ä–∞–Ω–∏ –ø–æ–¥–∞—Ç–æ—Ü–∏)

–ü–æ–¥–µ–ª–±–∞ –µ –Ω–∞–ø—Ä–∞–≤–µ–Ω–∞ –ø—Ä–∞–≤–∏–ª–Ω–æ –∏ —Ñ–µ—Ä

üìå –†–µ–∑—É–ª—Ç–∞—Ç: –ß–∏—Å—Ç–∏, –Ω—É–º–µ—Ä–∏—á–∫–∏, —Å–∫–∞–ª–∏—Ä–∞–Ω–∏ –ø–æ–¥–∞—Ç–æ—Ü–∏ —Å–ø—Ä–µ–º–Ω–∏ –∑–∞ PyTorch.

2Ô∏è‚É£ ChurnDataset
üîπ –ë–∞—Ä–∞—ö–∞

–î–∞ —Å–µ –Ω–∞—Å–ª–µ–¥–∏ torch.utils.data.Dataset

X –¥–∞ –±–∏–¥–µ:

float32 tensor

y –¥–∞ –±–∏–¥–µ:

float32

shape (N, 1)

–î–∞ –∏–º–∞:

__len__

__getitem__

‚úÖ –®—Ç–æ –Ω–∞–ø—Ä–∞–≤–∏–≤–º–µ

NumPy ‚Üí PyTorch tensors

y –≥–æ reshape-–Ω–∞–≤–º–µ –≤–æ (N,1) (–≤–∞–∂–Ω–æ –∑–∞ BCE loss)

Dataset –º–æ–∂–µ –¥–∞ —Å–µ –∫–æ—Ä–∏—Å—Ç–∏ —Å–æ DataLoader

üìå –†–µ–∑—É–ª—Ç–∞—Ç: –ü–æ–¥–∞—Ç–æ—Ü–∏ —Å–ø—Ä–µ–º–Ω–∏ –∑–∞ batch training.

3Ô∏è‚É£ build_model(input_dim)
üîπ –ë–∞—Ä–∞—ö–∞

–î–∞ —Å–µ –∫–æ—Ä–∏—Å—Ç–∏ nn.Sequential

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:

Linear ‚Üí ReLU ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí Sigmoid


–ü–æ—Å–ª–µ–¥–µ–Ω —Å–ª–æ—ò:

1 neuron

Sigmoid activation

‚úÖ –®—Ç–æ –Ω–∞–ø—Ä–∞–≤–∏–≤–º–µ

–ù–∞–ø—Ä–∞–≤–∏–≤–º–µ feed-forward neural network

Sigmoid –¥–∞–≤–∞ output –≤–æ [0,1] ‚Üí –≤–µ—Ä–æ—ò–∞—Ç–Ω–æ—Å—Ç –∑–∞ churn

üìå –†–µ–∑—É–ª—Ç–∞—Ç: Binary classifier.

4Ô∏è‚É£ train_one_epoch(...)
üîπ –ë–∞—Ä–∞—ö–∞

model.train()

Loop –ø—Ä–µ–∫—É batches

Forward pass

Loss calculation

zero_grad ‚Üí backward ‚Üí optimizer.step

–î–∞ —Å–µ –≤—Ä–∞—Ç–∏ –ø—Ä–æ—Å–µ—á–µ–Ω loss

‚úÖ –®—Ç–æ –Ω–∞–ø—Ä–∞–≤–∏–≤–º–µ

–ú–æ–¥–µ–ª–æ—Ç —É—á–∏ –ø—Ä–µ–∫—É backpropagation

Loss —Å–µ —Å–æ–±–∏—Ä–∞ –∏ —Å–µ –¥–µ–ª–∏ —Å–æ –±—Ä–æ—ò–æ—Ç –Ω–∞ batches

üìå –†–µ–∑—É–ª—Ç–∞—Ç: –ï–¥–µ–Ω —Ü–µ–ª–æ—Å–µ–Ω training epoch.

5Ô∏è‚É£ evaluate(model, val_loader)
üîπ –ë–∞—Ä–∞—ö–∞

model.eval()

torch.no_grad()

Threshold = 0.5

–î–∞ —Å–µ –≤—Ä–∞—Ç–∏ accuracy

‚úÖ –®—Ç–æ –Ω–∞–ø—Ä–∞–≤–∏–≤–º–µ

–ì–æ —Å—Ç–æ–ø–∏—Ä–∞–≤–º–µ gradient tracking

Sigmoid output ‚Üí 0 –∏–ª–∏ 1

–°–µ —Å–ø–æ—Ä–µ–¥—É–≤–∞ —Å–æ –≤–∏—Å—Ç–∏–Ω—Å–∫–∏—Ç–µ labels

üìå –†–µ–∑—É–ª—Ç–∞—Ç: –ú–µ—Ä–∫–∞ –∫–æ–ª–∫—É –¥–æ–±—Ä–æ –º–æ–¥–µ–ª–æ—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∏—Ä–∞.

6Ô∏è‚É£ Training loop (10 epochs)
üîπ –®—Ç–æ –≥–ª–µ–¥–∞–º–µ?

Loss ‚Üì –æ–¥ 0.54 ‚Üí 0.35

Val accuracy ‚Üë –æ–¥ 0.79 ‚Üí 0.8575

üìå –ó–Ω–∞—á–µ—ö–µ:
–ú–æ–¥–µ–ª–æ—Ç —É—á–∏ —Å—Ç–∞–±–∏–ª–Ω–æ –∏ –Ω–µ overfit-–∏—Ä–∞.

7Ô∏è‚É£ –û–¥–≥–æ–≤–æ—Ä–∏ –Ω–∞ —Ç–µ–æ—Ä–µ—Ç—Å–∫–∏—Ç–µ –ø—Ä–∞—à–∞—ö–∞
‚úî Validation accuracy?

‚âà 85.8%

‚úî Overfitting / Underfitting?

‚ùå –ù–∏—Ç—É –µ–¥–Ω–æ—Ç–æ ‚Äì –º–æ–¥–µ–ª–æ—Ç –µ –¥–æ–±—Ä–æ –±–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω

‚úî –ö–∞–∫–æ –¥–∞ —Å–µ –ø–æ–¥–æ–±—Ä–∏?

–ü–æ–¥–ª–∞–±–æ–∫–∞ –º—Ä–µ–∂–∞

Dropout

Feature engineering
